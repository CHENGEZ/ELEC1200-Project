# **ELEC1200 Project Report**
## **PART I**
### **Description**
The first part of the project is a simple speech recognition program. Two chosen single syllabus English words, “bar” and “door”, are stored in two .wav files “word1.wav” and “word2.wav” respectively. When the program is executed, it will prompt the user to choose between “loading one of the existing .wav files” and “speaking and recording on the scene”. Then the program will output which word was loaded or was spoken by the user. This is only a binary word recognition program, as it can only decide whether “it’s word1 or word2”. It is unable to detect and decode speech signal into any arbitrary English word.
### **Technique/Algorithm**
The main idea of this word recognition program is to make use of the difference between frequency spectrums of different words. The three important pieces of data are the signal of “word1.wav”, the signal of “word2.wav” and the signal originated from the user’s decision (Either a speech signal recorded on the scene, or the same as one of word1.wav and word2.wav), all of which are represented in the time domain. I will preform the Fourier Transform on all of the three signals using the `fft()` function provided by MATLAB in order to obtain the frequency spectra of the three signals. As the result of the Fourier Transform includes both the magnitude and phase, yet I am only interested in the magnitude, I will add `abs()` to only make use of the magnitude of their spectra. After this, I will have three different spectra, one for “bar”, one for “door” and one for the spoken/loaded word. Technically, now if we plot the three results of `abs(fft(signal))`, the horizontal axes corresponds to the frequency components, and the vertical axes corresponds to the corresponding amplitude of each frequency components. By visual inspection of the plots, we can already tell the difference between them in terms of where the maximum amplitude occurs. To simplify the comparison, I will not focus on the exact frequency where the maximum amplitude occurs. In other words, the exact value of the dominant frequency component of the signal is not important in terms of determining which word was spoken/loaded. Instead, as essentially the data of the three signals of stores in vectors/arrays in MATLAB, I will mainly focus on the index where the max amplitude occurs in the array. The index corresponding to the dominant frequency component of “bar”, “car” and the spoken/loaded word is stored in `ind_bar`, `ind_door` and `ind` respectively by using 
```
[max_mag_of_bar, ind_bar] = max(FT_of_bar); 
[max_mag_of_door, ind_door] = max(FT_of_door);
[max_mag, ind] = max(FT_of_y);
```
Then the decision of the program will be made based on the numerical difference between `ind` and `ind_door` and the numerical difference between `ind` and `ind_bar`. The two numerical differences are stored in the variables `diff_between_bar` and `diff_between_door` respectively. Whichever returns a smaller value suggests that the spoken/recorded word is “more similar” to that prerecorded word. If the user chose to load one of the existing files, then one of the index difference will simply be 0. 
### **Testing and Accuracy**
Since if the user’s decision was to load one of the existing files, the index difference between the loaded file with itself must be zero. There is no point testing this scenario as it’s just impossible to have any mistakes in terms of determining which word was loaded. The testing focus was on making decisions according to the recording on the scene. I used my own voice to randomly say one of the two words several times and check both the eventual output and the value of `diff_between_bar` and `diff_between_door` in the MATLAB workspace. The bigger the difference between `diff_between_bar` and `diff_between_door`, the better the detection performance will be. In terms of eventual output, no error was detected during my testing. But the difference between `diff_between_bar` and `diff_between_door` is quite unstable. Sometimes their difference is quite significant, which suggests that the recorded sound is extremely similar to one of the words and differs a lot from the other word. Yet there are occasions where the difference is not large, indicating that the recorded wave lies approximately in the middle between the two known data waves. This issue should be caused by the fact that even the same person repeatedly pronounces the same word for several times, each time the spectrum of his or her sound may not be the same. Sometimes there’s an overall shift to the high frequency direction, sometimes the opposite. Limited to a binary decision, if the frequency shift due to human voice each time isn’t large enough to make the gap reverse to the opposite direction, we can still obtain the correct output. But this indicates that the recognition accuracy may depend on the speakers tone, which means a high accuracy when the speaker is same as the person who recorded the data waves may not guarantee a high accuracy when the speaker is different.
## **PART II**
### **Description**

### **problems/issues found**